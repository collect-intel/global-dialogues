# calculate_pri.py

import pandas as pd
import numpy as np
import warnings
import argparse
import time
import sys
from datetime import datetime

def parse_args():
    """Parse command line arguments for the script."""
    parser = argparse.ArgumentParser(description='Calculate Participant Reliability Index (PRI) scores.')
    parser.add_argument('gd_number', type=int, help='The Global Dialogue number (e.g. 1, 2, 3)')
    parser.add_argument('--debug', action='store_true', help='Enable verbose debug output')
    parser.add_argument('--limit', type=int, help='Limit processing to first N participants (for testing)', default=None)
    return parser.parse_args()

# --- Constants and Configuration ---
def get_constants(gd_number):
    """Define constants and file paths based on GD number."""
    DATA_DIR = f"Data/GD{gd_number}"
    TAGS_DIR = f"{DATA_DIR}/tags"

    # File paths
    AGGREGATE_STD_PATH = f"{DATA_DIR}/GD{gd_number}_aggregate_standardized.csv"
    BINARY_PATH = f"{DATA_DIR}/GD{gd_number}_binary.csv"
    PREFERENCE_PATH = f"{DATA_DIR}/GD{gd_number}_preference.csv"
    VERBATIM_MAP_PATH = f"{DATA_DIR}/GD{gd_number}_verbatim_map.csv"
    THOUGHT_LABELS_PATH = f"{TAGS_DIR}/all_thought_labels.csv"

    # Signal Thresholds (adjusted for better coverage)
    ASC_HIGH_THRESHOLD = 0.70  # Agreement rate for strong agreement (lowered from 0.80)
    ASC_LOW_THRESHOLD = 0.30   # Agreement rate for strong disagreement (raised from 0.20)
    UNIVERSAL_DISAGREEMENT_THRESHOLD = 0.25  # Max agreement rate for a response to be considered 'disagreed' (raised from 0.20)
    UNIVERSAL_DISAGREEMENT_COVERAGE = 0.80  # Minimum proportion of population needed for 'universal' disagreement (lowered from 0.90)
    
    return {
        'DATA_DIR': DATA_DIR,
        'TAGS_DIR': TAGS_DIR,
        'AGGREGATE_STD_PATH': AGGREGATE_STD_PATH,
        'BINARY_PATH': BINARY_PATH,
        'PREFERENCE_PATH': PREFERENCE_PATH,
        'VERBATIM_MAP_PATH': VERBATIM_MAP_PATH,
        'THOUGHT_LABELS_PATH': THOUGHT_LABELS_PATH,
        'ASC_HIGH_THRESHOLD': ASC_HIGH_THRESHOLD,
        'ASC_LOW_THRESHOLD': ASC_LOW_THRESHOLD,
        'UNIVERSAL_DISAGREEMENT_THRESHOLD': UNIVERSAL_DISAGREEMENT_THRESHOLD,
        'UNIVERSAL_DISAGREEMENT_COVERAGE': UNIVERSAL_DISAGREEMENT_COVERAGE
    }

# --- Data Loading and Cleaning ---
def load_and_clean_data(config, debug=False):
    """Loads and performs initial cleaning on necessary CSV files."""
    print(f"Loading data from {config['DATA_DIR']}...")
    
    # Configure pandas to display more information for debug
    if debug:
        pd.set_option('display.max_columns', 10)
        pd.set_option('display.width', 120)

    # --- Load Binary ---
    binary_required_cols = ['Participant ID', 'Thought ID', 'Vote', 'Timestamp']
    try:
        binary_df = pd.read_csv(
            config['BINARY_PATH'],
            delimiter=',',
            quotechar='"',
            on_bad_lines='warn',
            low_memory=False,
            header=0
        )
        print(f"Loaded binary data with shape: {binary_df.shape}")
        
        missing_cols = [col for col in binary_required_cols if col not in binary_df.columns]
        if missing_cols:
            print(f"Warning: Missing required columns in binary file: {missing_cols}")
            for col in missing_cols:
                binary_df[col] = np.nan
        
        # Convert timestamps to datetime and normalize vote values
        if 'Timestamp' in binary_df.columns:
            # Convert timestamps with proper format parsing to avoid warnings
            try:
                # First try with a specific format
                binary_df['Timestamp'] = pd.to_datetime(binary_df['Timestamp'], 
                                                      format='%B %d, %Y at %I:%M %p (GMT)',
                                                      errors='coerce')
            except:
                # Fallback to flexible parsing if specific format fails
                binary_df['Timestamp'] = pd.to_datetime(binary_df['Timestamp'], errors='coerce')
        
        if 'Vote' in binary_df.columns:
            # Handle various vote format possibilities
            binary_df['VoteNumeric'] = binary_df['Vote'].map(
                {'Agree': 1, 'agree': 1, 'Disagree': 0, 'disagree': 0}
            ).astype(float)
            
            # Report conversion stats
            if debug:
                total = len(binary_df)
                converted = binary_df['VoteNumeric'].notna().sum()
                print(f"Vote conversion: {converted}/{total} votes successfully converted to numeric")
        else:
            binary_df['VoteNumeric'] = np.nan
    except Exception as e:
        print(f"Error loading or processing binary file: {e}")
        binary_df = pd.DataFrame(columns=binary_required_cols + ['VoteNumeric'])

    # --- Load Preference ---
    preference_required_cols = ['Participant ID', 'Timestamp']
    try:
        preference_df = pd.read_csv(
            config['PREFERENCE_PATH'],
            delimiter=',',
            quotechar='"',
            on_bad_lines='warn',
            low_memory=False,
            header=0
        )
        print(f"Loaded preference data with shape: {preference_df.shape}")
        
        missing_cols = [col for col in preference_required_cols if col not in preference_df.columns]
        if missing_cols:
            print(f"Warning: Missing required columns in preference file: {missing_cols}")
            for col in missing_cols:
                preference_df[col] = np.nan
        
        # Convert timestamps to datetime
        if 'Timestamp' in preference_df.columns:
            try:
                # First try with a specific format
                preference_df['Timestamp'] = pd.to_datetime(preference_df['Timestamp'], 
                                                          format='%B %d, %Y at %I:%M %p (GMT)',
                                                          errors='coerce')
            except:
                # Fallback to flexible parsing if specific format fails
                preference_df['Timestamp'] = pd.to_datetime(preference_df['Timestamp'], errors='coerce')
    except Exception as e:
        print(f"Error loading or processing preference file: {e}")
        preference_df = pd.DataFrame(columns=preference_required_cols)

    # --- Load Thought Labels ---
    thought_labels_required_cols = ['Participant ID', 'Question ID']
    try:
        thought_labels_df = pd.read_csv(
            config['THOUGHT_LABELS_PATH'],
            low_memory=False,
            encoding='utf-8-sig'
        )
        thought_labels_df.columns = thought_labels_df.columns.str.strip()
        print(f"Loaded thought labels data with shape: {thought_labels_df.shape}")
        
        missing_cols = [col for col in thought_labels_required_cols if col not in thought_labels_df.columns]
        if missing_cols:
            print(f"Warning: Missing required columns in thought labels file: {missing_cols}")
    except Exception as e:
        print(f"Error loading or processing thought labels file: {e}")
        thought_labels_df = pd.DataFrame(columns=thought_labels_required_cols)

    # --- Load Verbatim Map ---
    verbatim_map_required_cols = ['Question ID', 'Question Text', 'Participant ID', 'Thought ID', 'Thought Text']
    try:
        print(f"Loading verbatim map file...")
        verbatim_map_df = pd.read_csv(
            config['VERBATIM_MAP_PATH'],
            delimiter=',',
            quotechar='"',
            on_bad_lines='warn',
            header=0,
            encoding='utf-8',
            engine='python'  # More robust for complex CSV files
        )
        print(f"Loaded verbatim map data with shape: {verbatim_map_df.shape}")
        
        # Basic validation
        missing_cols = [col for col in verbatim_map_required_cols if col not in verbatim_map_df.columns]
        if missing_cols:
            print(f"Warning: Missing required columns in verbatim map file: {missing_cols}")
    except Exception as e:
        print(f"Error loading verbatim map file: {e}")
        verbatim_map_df = pd.DataFrame(columns=verbatim_map_required_cols)

    # --- Load Aggregate Standardized ---
    aggregate_std_required_cols = ['All', 'Participant ID', 'Question ID']
    try:
        warnings.simplefilter(action='ignore', category=pd.errors.DtypeWarning)
        
        # First, try to read the first few rows to see what's in the file
        if debug:
            try:
                sample_df = pd.read_csv(
                    config['AGGREGATE_STD_PATH'],
                    low_memory=False,
                    nrows=5
                )
                print(f"Sample of aggregate standardized data (first 5 rows, first 5 columns):")
                print(sample_df.iloc[:5, :5])
            except Exception as e:
                print(f"Error reading sample from aggregate file: {e}")
            
        # Now read the full file, being explicit about types
        aggregate_std_df = pd.read_csv(
            config['AGGREGATE_STD_PATH'],
            low_memory=False,
            dtype={
                'Question ID': str,
                'Question Type': str,
                'Question': str,
                'Response': str,
                'All': str,  # Keep percentage columns as strings initially
                'Participant ID': str
            }
        )
        
        # Create a copy to avoid fragmentation
        aggregate_std_df = aggregate_std_df.copy()
        warnings.simplefilter(action='default', category=pd.errors.DtypeWarning)
        print(f"Loaded aggregate data with shape: {aggregate_std_df.shape}")
        
        # Check for required columns
        missing_cols = [col for col in aggregate_std_required_cols if col not in aggregate_std_df.columns]
        if missing_cols:
            print(f"Warning: Missing required columns in aggregate file: {missing_cols}")
            aggregate_std_df = pd.DataFrame(columns=['All_Agreement', 'Author Participant ID', 'Question ID', 'Thought ID'])
        else:
            print(f"Found required columns in aggregate file")
            
            # Rename participant ID column for clarity
            aggregate_std_df.rename(columns={'Participant ID': 'Author Participant ID'}, inplace=True)
            
            # Add placeholder columns that might be needed later
            aggregate_std_df['Thought ID'] = np.nan
            
            # Convert percentage strings to numeric values
            def safe_pct_to_float(x):
                """Safely convert percentage strings to float values."""
                if pd.isna(x):
                    return np.nan
                try:
                    # First check if it's already a numeric value
                    if isinstance(x, (int, float)):
                        return float(x) / 100.0 if x > 1 else float(x)
                    # Then try to convert from string
                    x_str = str(x).strip()
                    if x_str.endswith('%'):
                        return float(x_str.rstrip('%')) / 100.0
                    # Handle dash or other non-numeric placeholders
                    elif x_str in ['-', ' - ']:
                        return np.nan
                    else:
                        # If no % sign, try direct conversion but check range
                        val = float(x_str)
                        return val / 100.0 if val > 1 else val
                except (ValueError, TypeError) as e:
                    if debug:
                        print(f"Warning: Could not convert value '{x}' to float: {e}")
                    return np.nan
            
            # Convert the 'All' column to numeric
            aggregate_std_df['All_Agreement'] = aggregate_std_df['All'].apply(safe_pct_to_float)
            
            # Sample conversions for debug
            if debug:
                sample_values = aggregate_std_df['All'].head(5).tolist()
                sample_converted = [safe_pct_to_float(x) for x in sample_values]
                print(f"Sample conversion of 'All' column:")
                for orig, conv in zip(sample_values, sample_converted):
                    print(f"  '{orig}' -> {conv}")
    except Exception as e:
        print(f"Error loading or processing aggregate file: {e}")
        aggregate_std_df = pd.DataFrame(columns=['All_Agreement', 'Author Participant ID', 'Question ID', 'Thought ID'])

    # Get unique participant IDs from binary votes (most comprehensive source)
    all_participant_ids = binary_df['Participant ID'].unique()
    print(f"Found {len(all_participant_ids)} unique participants")

    return binary_df, preference_df, thought_labels_df, verbatim_map_df, aggregate_std_df, all_participant_ids


# --- Signal Calculation Functions ---

def calculate_duration(participant_id, binary_times_df, preference_times_df, debug=False):
    """
    Calculates survey duration for a participant based on timestamps of their activity.
    
    Args:
        participant_id: Unique ID of the participant
        binary_times_df: DataFrame with binary vote timestamps
        preference_times_df: DataFrame with preference judgment timestamps
        debug: Whether to print debug information
        
    Returns:
        pd.Timedelta: Duration between first and last recorded activity
    """
    if debug:
        print(f"[Duration {participant_id}] Calculating duration...")
    
    # Filter timestamps for the participant
    participant_binary_times = binary_times_df[binary_times_df['Participant ID'] == participant_id]['Timestamp'].dropna()
    participant_pref_times = preference_times_df[preference_times_df['Participant ID'] == participant_id]['Timestamp'].dropna()

    # Combine all timestamps
    all_times = pd.concat([participant_binary_times, participant_pref_times])

    if len(all_times) < 2:
        if debug:
            print(f"[Duration {participant_id}] Insufficient timestamps (found {len(all_times)})")
        return pd.Timedelta(seconds=0)

    # Calculate time difference between first and last activity
    duration = all_times.max() - all_times.min()
    
    if debug:
        print(f"[Duration {participant_id}] First activity: {all_times.min()}")
        print(f"[Duration {participant_id}] Last activity: {all_times.max()}")
        print(f"[Duration {participant_id}] Duration: {duration}")
    
    return duration


def calculate_low_quality_tag_perc(participant_id, thought_labels_df, debug=False):
    """
    Calculates the percentage of a participant's responses tagged as 'Uninformative Answer'.
    
    Args:
        participant_id: Unique ID of the participant
        thought_labels_df: DataFrame with thought labels
        debug: Whether to print debug information
        
    Returns:
        float: Ratio of low quality responses to total responses (0-1)
    """
    if debug:
        print(f"[LowQuality {participant_id}] Calculating low quality percentage...")
    
    # Filter for this participant's labeled responses
    participant_labels = thought_labels_df[thought_labels_df['Participant ID'] == participant_id]

    if participant_labels.empty:
        if debug:
            print(f"[LowQuality {participant_id}] No labeled responses found")
        return 0.0  # No labeled responses, assume perfect quality

    # Find tag columns
    tag_cols = [col for col in thought_labels_df.columns if col.startswith('Tag ')]
    if debug:
        print(f"[LowQuality {participant_id}] Found {len(tag_cols)} tag columns")
    
    if not tag_cols:
        if debug:
            print(f"[LowQuality {participant_id}] No tag columns found")
        return 0.0  # No tag columns, cannot evaluate
    
    # Check for 'Uninformative Answer' tag in any tag column
    is_low_quality = participant_labels[tag_cols].apply(
        lambda row: any('Uninformative Answer' == val for val in row.values if pd.notna(val)), 
        axis=1
    )

    num_low_quality = is_low_quality.sum()
    total_responses = len(participant_labels)
    
    if debug:
        print(f"[LowQuality {participant_id}] Low quality responses: {num_low_quality}/{total_responses}")
    
    return num_low_quality / total_responses if total_responses > 0 else 0.0


def calculate_universal_disagreement_perc(participant_id, verbatim_map_df, aggregate_std_df, config, debug=False):
    """
    Calculates the percentage of a participant's responses that received widespread disagreement
    across major demographic segments.
    
    Args:
        participant_id: Unique ID of the participant
        verbatim_map_df: DataFrame mapping thoughts to participants
        aggregate_std_df: DataFrame with agreement scores
        config: Dictionary with configuration values
        debug: Whether to print debug information
        
    Returns:
        float: Ratio of universally disagreed responses to total responses (0-1)
    """
    if debug:
        print(f"[UniversalDisagreement {participant_id}] Calculating universal disagreement...")
    
    # Get thoughts authored by this participant
    authored_thought_ids = verbatim_map_df[verbatim_map_df['Participant ID'] == participant_id]['Thought ID'].unique()
    
    if debug:
        print(f"[UniversalDisagreement {participant_id}] Found {len(authored_thought_ids)} authored thoughts")
    
    if len(authored_thought_ids) == 0:
        return 0.0  # No authored thoughts, cannot evaluate
    
    # Get agreement rates for these thoughts from aggregate data
    authored_thoughts_data = []
    
    # Process each authored thought
    for thought_id in authored_thought_ids:
        # Get question ID for this thought
        question_row = verbatim_map_df[verbatim_map_df['Thought ID'] == thought_id]
        if question_row.empty:
            continue
            
        question_id = question_row['Question ID'].iloc[0]
        
        # Find agreement data for this question in aggregate
        question_data = aggregate_std_df[aggregate_std_df['Question ID'] == question_id]
        if question_data.empty:
            continue
            
        # Get all segment columns that end with numeric values (agreement rates)
        segment_cols = [col for col in aggregate_std_df.columns if col.startswith(('O1:', 'O2:', 'O3:', 'O4:', 'O5:', 'O6:', 'O7:'))]
        
        if not segment_cols:
            if debug:
                print(f"[UniversalDisagreement {participant_id}] No segment columns found for analysis")
            continue
            
        # For simplicity, we'll use the 'All_Agreement' as a proxy
        # In a full implementation, we would check agreement across segments
        agreement_value = question_data['All_Agreement'].iloc[0] if 'All_Agreement' in question_data.columns else None
        
        if agreement_value is not None and not pd.isna(agreement_value):
            authored_thoughts_data.append({
                'Thought ID': thought_id,
                'Question ID': question_id,
                'All_Agreement': agreement_value
            })
    
    if not authored_thoughts_data:
        if debug:
            print(f"[UniversalDisagreement {participant_id}] No agreement data found for authored thoughts")
        return 0.0
        
    # Create DataFrame with thought agreement data
    authored_aggr_df = pd.DataFrame(authored_thoughts_data)
    
    # Check how many thoughts have universal disagreement (below threshold)
    is_universally_disagreed = authored_aggr_df['All_Agreement'] < config['UNIVERSAL_DISAGREEMENT_THRESHOLD']
    num_universally_disagreed = is_universally_disagreed.sum()
    total_evaluated = len(authored_aggr_df)
    
    if debug:
        print(f"[UniversalDisagreement {participant_id}] Universally disagreed: {num_universally_disagreed}/{total_evaluated}")
    
    return num_universally_disagreed / total_evaluated if total_evaluated > 0 else 0.0


def compute_consensus_data(aggregate_std_df, verbatim_map_df, config, debug=False):
    """
    Pre-compute consensus data for all thoughts to be used in ASC calculations.
    This is a performance optimization that avoids recomputing this data for each participant.
    
    Args:
        aggregate_std_df: DataFrame with agreement scores
        verbatim_map_df: DataFrame mapping thoughts to participants
        config: Dictionary with configuration values
        debug: Whether to print debug information
        
    Returns:
        dict: Dictionary with sets of strong agreement and disagreement thought IDs
    """
    start_time = time.time()
    
    if debug:
        print(f"Pre-computing consensus data for all thoughts...")
    
    high_threshold = config['ASC_HIGH_THRESHOLD']
    low_threshold = config['ASC_LOW_THRESHOLD']
    
    # Create a mapping of Question ID to Agreement Score
    question_agreement_map = {}
    
    # Additional debugging for aggregate data shape
    if debug:
        print(f"Aggregate data shape: {aggregate_std_df.shape}")
        print(f"Sample of aggregate data (first 5 rows):")
        if 'All_Agreement' in aggregate_std_df.columns:
            print(aggregate_std_df[['Question ID', 'All_Agreement']].head(5))
        else:
            print(f"Warning: 'All_Agreement' column not found. Available columns: {aggregate_std_df.columns.tolist()}")
    
    # Process each question in aggregate data
    agreement_count = 0
    for _, row in aggregate_std_df.iterrows():
        if 'Question ID' in row and 'All_Agreement' in row and pd.notna(row['All_Agreement']):
            question_id = row['Question ID']
            agreement = row['All_Agreement']
            agreement_count += 1
            
            # For each question, store the agreement score
            # If we have multiple rows for the same question, use the highest agreement
            if question_id in question_agreement_map:
                question_agreement_map[question_id] = max(question_agreement_map[question_id], agreement)
            else:
                question_agreement_map[question_id] = agreement
    
    if debug:
        print(f"Found {agreement_count} rows with valid agreement data")
        print(f"Mapped agreement scores for {len(question_agreement_map)} unique questions")
        # Show distribution of agreement scores
        if question_agreement_map:
            scores = list(question_agreement_map.values())
            print(f"Agreement score distribution:")
            print(f"  Min: {min(scores):.3f}")
            print(f"  Max: {max(scores):.3f}")
            print(f"  Mean: {sum(scores)/len(scores):.3f}")
            print(f"  Scores > {high_threshold}: {sum(1 for s in scores if s >= high_threshold)}")
            print(f"  Scores < {low_threshold}: {sum(1 for s in scores if s <= low_threshold)}")
    
    # Create mappings for thought ID to agreement score and consensus status
    strong_agree_thoughts = set()
    strong_disagree_thoughts = set()
    
    # Create a mapping of thought ID to question ID for quick lookups
    thought_to_question = dict(zip(verbatim_map_df['Thought ID'], verbatim_map_df['Question ID']))
    
    if debug:
        print(f"Verbatim map contains {len(thought_to_question)} thought-to-question mappings")
    
    # Process each thought and assign it to the appropriate consensus category
    mapped_thoughts = 0
    for thought_id, question_id in thought_to_question.items():
        if question_id in question_agreement_map:
            mapped_thoughts += 1
            agreement = question_agreement_map[question_id]
            
            if agreement >= high_threshold:
                strong_agree_thoughts.add(thought_id)
            elif agreement <= low_threshold:
                strong_disagree_thoughts.add(thought_id)
    
    if debug and mapped_thoughts < len(thought_to_question):
        print(f"Warning: Only {mapped_thoughts}/{len(thought_to_question)} thoughts could be mapped to agreement scores")
    
    # Create a reusable dictionary of consensus data
    consensus_data = {
        'strong_agree_thoughts': strong_agree_thoughts,
        'strong_disagree_thoughts': strong_disagree_thoughts
    }
    
    if debug:
        print(f"Identified {len(strong_agree_thoughts)} thoughts with strong agreement (≥{high_threshold})")
        print(f"Identified {len(strong_disagree_thoughts)} thoughts with strong disagreement (≤{low_threshold})")
        print(f"Total consensus thoughts: {len(strong_agree_thoughts) + len(strong_disagree_thoughts)}")
        print(f"Consensus data computation completed in {time.time() - start_time:.2f} seconds")
    
    return consensus_data


def calculate_asc_score(participant_id, binary_df, consensus_data, debug=False):
    """
    Calculates the Anti-Social Consensus (ASC) score - the rate at which the participant votes
    against strong consensus items.
    
    Args:
        participant_id: Unique ID of the participant
        binary_df: DataFrame with binary votes
        consensus_data: Pre-computed consensus data dictionary
        debug: Whether to print debug information
        
    Returns:
        float: Ratio of votes against consensus to total votes on consensus items (0-1)
    """
    if debug:
        print(f"[ASC {participant_id}] Calculating Anti-Social Consensus score...")
    
    # Extract pre-computed consensus data
    strong_agree_thoughts = consensus_data['strong_agree_thoughts']
    strong_disagree_thoughts = consensus_data['strong_disagree_thoughts']
    
    # Create a set of all consensus thoughts for faster lookups
    all_consensus_thoughts = strong_agree_thoughts.union(strong_disagree_thoughts)
    
    if not all_consensus_thoughts:
        if debug:
            print(f"[ASC {participant_id}] No consensus thoughts found for analysis")
        return np.nan  # Cannot evaluate ASC without consensus thoughts
    
    # Get participant's votes on consensus thoughts (filter binary_df for this participant)
    participant_binary_df = binary_df[binary_df['Participant ID'] == participant_id]
    
    # Only keep votes on consensus thoughts
    consensus_votes = participant_binary_df[participant_binary_df['Thought ID'].isin(all_consensus_thoughts)]
    
    if consensus_votes.empty:
        if debug:
            print(f"[ASC {participant_id}] Participant did not vote on any consensus thoughts")
        return np.nan  # Cannot evaluate if no votes on consensus thoughts
    
    # Count votes AGAINST consensus
    against_consensus_count = 0
    total_consensus_votes = 0
    
    # Process each vote
    for _, vote_row in consensus_votes.iterrows():
        thought_id = vote_row['Thought ID']
        vote_value = vote_row['VoteNumeric']
        
        # Skip if vote is not clearly agree (1) or disagree (0)
        if pd.isna(vote_value):
            continue
            
        total_consensus_votes += 1
            
        # Check if vote is against consensus
        if thought_id in strong_agree_thoughts and vote_value == 0:  # Disagreed with high consensus
            against_consensus_count += 1
        elif thought_id in strong_disagree_thoughts and vote_value == 1:  # Agreed with low consensus
            against_consensus_count += 1
    
    if total_consensus_votes == 0:
        if debug:
            print(f"[ASC {participant_id}] No valid consensus votes found")
        return np.nan
    
    asc_score = against_consensus_count / total_consensus_votes
    
    if debug:
        print(f"[ASC {participant_id}] Against consensus votes: {against_consensus_count}/{total_consensus_votes}")
        print(f"[ASC {participant_id}] ASC score: {asc_score}")
    
    return asc_score


# --- Main Processing ---

def calculate_all_pri_signals(binary_df, preference_df, thought_labels_df, verbatim_map_df, aggregate_std_df, config, participant_limit=None, debug=False):
    """
    Calculates all PRI signals for all participants.
    
    Args:
        binary_df: DataFrame containing binary vote data
        preference_df: DataFrame containing preference judgment data
        thought_labels_df: DataFrame containing response quality tags
        verbatim_map_df: DataFrame mapping responses to participants
        aggregate_std_df: DataFrame containing standardized aggregate data
        config: Dictionary with configuration values
        participant_limit: Limit processing to first N participants (for testing)
        debug: Whether to print debug information
        
    Returns:
        DataFrame containing calculated PRI signals for each participant
    """
    print("\nCalculating PRI signals for all participants...")
    
    # Get unique participant IDs from a comprehensive source (binary votes)
    all_participant_ids = binary_df['Participant ID'].unique()
    
    # Apply limit if specified
    if participant_limit is not None:
        participant_limit = min(participant_limit, len(all_participant_ids))
        all_participant_ids = all_participant_ids[:participant_limit]
        print(f"Limited to first {participant_limit} participants")
    else:
        participant_limit = len(all_participant_ids)
        
    print(f"Processing {participant_limit} participants...")

    # Pre-filter timestamp data for efficiency
    binary_times_df = binary_df[['Participant ID', 'Timestamp']].copy()
    preference_times_df = preference_df[['Participant ID', 'Timestamp']].copy()

    # Process each participant
    results = []
    
    # Use for progress reporting
    progress_step = max(1, participant_limit // 10)
    start_time = time.time()
    
    # Pre-compute consensus data for ASC calculation
    print("Pre-computing consensus data for ASC calculation...")
    consensus_data = compute_consensus_data(aggregate_std_df, verbatim_map_df, config, debug)

    for i, participant_id in enumerate(all_participant_ids):
        if i % progress_step == 0 or i == participant_limit - 1:
            elapsed = time.time() - start_time
            avg_time = elapsed / (i + 1) if i > 0 else 0
            est_remaining = avg_time * (participant_limit - i - 1)
            print(f"Processing participant {i+1}/{participant_limit} ({i/participant_limit*100:.1f}%)... " +
                  f"(Est. remaining: {est_remaining/60:.1f} minutes)")
        
        # Calculate metrics
        try:
            # 1. Duration
            duration = calculate_duration(participant_id, binary_times_df, preference_times_df, debug)
            
            # 2. Low Quality Tags Percentage
            low_quality_perc = calculate_low_quality_tag_perc(participant_id, thought_labels_df, debug)
            
            # 3. Universal Disagreement Percentage
            universal_disagreement_perc = calculate_universal_disagreement_perc(
                participant_id, verbatim_map_df, aggregate_std_df, config, debug
            )
            
            # 4. Anti-Social Consensus Score (raw - lower is better)
            asc_raw = calculate_asc_score(
                participant_id, binary_df, consensus_data, debug
            )
            
            # Add results
            results.append({
                'Participant ID': participant_id,
                'Duration_seconds': duration.total_seconds() if pd.notna(duration) else np.nan,
                'LowQualityTag_Perc': low_quality_perc,
                'UniversalDisagreement_Perc': universal_disagreement_perc,
                'ASC_Score_Raw': asc_raw,
            })
        except Exception as e:
            print(f"Error processing participant {participant_id}: {e}")
            # Add empty results to maintain participant count
            results.append({
                'Participant ID': participant_id,
                'Duration_seconds': np.nan,
                'LowQualityTag_Perc': np.nan,
                'UniversalDisagreement_Perc': np.nan,
                'ASC_Score_Raw': np.nan,
            })

    results_df = pd.DataFrame(results)
    print("Signal calculation complete.")
    return results_df


def normalize_and_calculate_pri(pri_signals_df, debug=False):
    """
    Normalizes the raw PRI signals and calculates the final PRI score.
    
    Args:
        pri_signals_df: DataFrame with raw PRI metrics
        debug: Whether to print debug information
        
    Returns:
        DataFrame with normalized metrics and final PRI score
    """
    print("\nNormalizing metrics and calculating final PRI scores...")
    
    # Check how many NaN values we have in each column
    if debug:
        print("\nNaN counts in raw signals:")
        print(pri_signals_df[['Duration_seconds', 'LowQualityTag_Perc', 'UniversalDisagreement_Perc', 'ASC_Score_Raw']].isna().sum())
    
    # Simple min-max normalization function
    def min_max_normalize(series, invert=False):
        """Min-max normalization with optional inversion"""
        if series.isna().all():
            return series  # Return as-is if all NaN
        
        # Replace NaN with median for normalization purposes
        median_val = series.median()
        filled_series = series.fillna(median_val)
        
        min_val = filled_series.min()
        max_val = filled_series.max()
        
        # Avoid division by zero
        if min_val == max_val:
            normalized = pd.Series(0.5, index=series.index)
        else:
            normalized = (filled_series - min_val) / (max_val - min_val)
            
        # Invert if needed (for metrics where lower raw value is better)
        if invert:
            normalized = 1 - normalized
            
        # Restore NaN values
        normalized[series.isna()] = np.nan
        
        return normalized
    
    # Normalize metrics
    
    # 1. Duration (longer duration is better)
    pri_signals_df['Duration_Norm'] = min_max_normalize(pri_signals_df['Duration_seconds'])
    
    # 2. Low Quality Tags (lower percentage is better, so invert)
    pri_signals_df['LowQualityTag_Norm'] = min_max_normalize(pri_signals_df['LowQualityTag_Perc'], invert=True)
    
    # 3. Universal Disagreement (lower percentage is better, so invert)
    pri_signals_df['UniversalDisagreement_Norm'] = min_max_normalize(pri_signals_df['UniversalDisagreement_Perc'], invert=True)
    
    # 4. Anti-Social Consensus (lower score is better, so invert)
    asc_available = not pri_signals_df['ASC_Score_Raw'].isna().all()
    
    if asc_available:
        # Normal calculation with ASC
        pri_signals_df['ASC_Norm'] = min_max_normalize(pri_signals_df['ASC_Score_Raw'], invert=True)
        
        # Define weights for each component
        weights = {
            'Duration_Norm': 0.2,
            'LowQualityTag_Norm': 0.3,
            'UniversalDisagreement_Norm': 0.3,
            'ASC_Norm': 0.2
        }
        
        # Calculate final weighted PRI score with all components
        pri_signals_df['PRI_Score'] = (
            pri_signals_df['Duration_Norm'] * weights['Duration_Norm'] +
            pri_signals_df['LowQualityTag_Norm'] * weights['LowQualityTag_Norm'] +
            pri_signals_df['UniversalDisagreement_Norm'] * weights['UniversalDisagreement_Norm'] +
            pri_signals_df['ASC_Norm'] * weights['ASC_Norm']
        )
    else:
        # Adjusted calculation without ASC
        print("Warning: No valid ASC scores available. Calculating PRI without ASC component.")
        
        # Adjust weights to distribute ASC's weight to other components
        adjusted_weights = {
            'Duration_Norm': 0.25,
            'LowQualityTag_Norm': 0.375,
            'UniversalDisagreement_Norm': 0.375
        }
        
        # Calculate final weighted PRI score without ASC
        pri_signals_df['PRI_Score'] = (
            pri_signals_df['Duration_Norm'] * adjusted_weights['Duration_Norm'] +
            pri_signals_df['LowQualityTag_Norm'] * adjusted_weights['LowQualityTag_Norm'] +
            pri_signals_df['UniversalDisagreement_Norm'] * adjusted_weights['UniversalDisagreement_Norm']
        )
    
    # Create a 1-5 scale version for easier interpretation
    pri_signals_df['PRI_Scale_1_5'] = pri_signals_df['PRI_Score'] * 4 + 1
    
    print("PRI score calculation complete.")
    return pri_signals_df


# --- Script Execution ---

if __name__ == "__main__":
    start_time = time.time()
    
    # Parse command line arguments
    args = parse_args()
    GD_NUMBER = args.gd_number
    DEBUG = args.debug
    PARTICIPANT_LIMIT = args.limit
    
    print(f"Calculating PRI for Global Dialogue {GD_NUMBER}")
    print(f"Debug mode: {'Enabled' if DEBUG else 'Disabled'}")
    if PARTICIPANT_LIMIT:
        print(f"Limiting to first {PARTICIPANT_LIMIT} participants for testing")
    
    # Get constants based on GD number
    config = get_constants(GD_NUMBER)
    
    # 1. Load and clean data
    binary_df, preference_df, thought_labels_df, verbatim_map_df, aggregate_std_df, all_participant_ids = load_and_clean_data(config, DEBUG)
    
    # Check if data loading was successful
    essential_dfs = [binary_df, preference_df]
    if any(df.empty for df in essential_dfs):
        print("Error: Essential data files could not be loaded. Exiting.")
        sys.exit(1)
    
    # 2. Calculate raw PRI signals
    pri_signals_df = calculate_all_pri_signals(
        binary_df, preference_df, thought_labels_df, verbatim_map_df, 
        aggregate_std_df, config, PARTICIPANT_LIMIT, DEBUG
    )
    
    # 3. Normalize signals and calculate final PRI
    pri_signals_df = normalize_and_calculate_pri(pri_signals_df, DEBUG)
    
    # 4. Print summary statistics
    print("\nPRI Score Statistics:")
    print(pri_signals_df[['PRI_Score', 'PRI_Scale_1_5']].describe())
    
    # 5. Show top/bottom participants
    print("\nTop 5 Most Reliable Participants:")
    print(pri_signals_df.sort_values('PRI_Score', ascending=False).head(5)[['Participant ID', 'PRI_Score', 'PRI_Scale_1_5']])
    
    print("\nBottom 5 Least Reliable Participants:")
    print(pri_signals_df.sort_values('PRI_Score', ascending=True).head(5)[['Participant ID', 'PRI_Score', 'PRI_Scale_1_5']])
    
    # 6. Save results to CSV
    output_path = f"{config['DATA_DIR']}/GD{GD_NUMBER}_pri_scores.csv"
    pri_signals_df.to_csv(output_path, index=False)
    print(f"\nResults saved to {output_path}")
    
    # 7. Print execution time
    end_time = time.time()
    elapsed_time = end_time - start_time
    print(f"\nExecution completed in {elapsed_time:.2f} seconds ({elapsed_time/60:.2f} minutes)")